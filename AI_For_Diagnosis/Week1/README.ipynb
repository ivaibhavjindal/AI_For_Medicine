{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "README.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzLWia0H9YMiyhNXHJ94NX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivaibhavjindal/AI_For_Medicine/blob/vaibhav/AI_For_Diagnosis/Week1/README.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7SR47uZRqpZ",
        "colab_type": "text"
      },
      "source": [
        "## AI for medicine. \n",
        "For example, given an\n",
        "image of a chest X-ray, so unstructured image data, can you train a neural network to diagnose whether or not\n",
        "a patient has pneumonia? Or given structure data such as the patient's\n",
        "lab results, can you train a decision tree to estimate the risk\n",
        "of heart attack? Working on these\n",
        "concrete problems, you also see a lot of\n",
        "the practical aspects of machine learning from how the deal with\n",
        "imbalanced data sets, to how to work with missing data, to picking the right\n",
        "evaluation metric.\n",
        "\n",
        "In machine learning,\n",
        "we often default to classification accuracy\n",
        "as the metric. But for many applications, that's not the right metric. So how do you choose a\n",
        "more appropriate one? Even if your current\n",
        "work is not a medicine, I think you'll find the\n",
        "application scenarios and the practice of these application scenarios really useful, and maybe it\n",
        "will convince you to get more\n",
        "interested in medicine.\n",
        "\n",
        "AI for medicine is taking off all around the\n",
        "world right now. So this is actually a\n",
        "great time for you to jump in and try to\n",
        "have a huge impact. Maybe you can be\n",
        "the one to invent something that saves a\n",
        "lot of patients lives.\n",
        "## AI for Medical Diagnosis\n",
        "It is about identifying disease. Diagnosis means, the\n",
        "process of determining which disease or condition explains the person's symptoms, signs, and medical results.\n",
        "## Prognosis\n",
        "Predicting the future\n",
        "health of the patients, which is called prognosis. You will learn how to work\n",
        "with structured data. So let's say, you have a patient's lab values\n",
        "and their demographics, and use those to predict\n",
        "the risk of an event, such as their risk of death or their risk of a heart attack.\n",
        "## Treatment\n",
        "The process of medical care and also for\n",
        "information extraction, getting information\n",
        "out of medical texts. You will\n",
        "learn how to use machine learning\n",
        "models to be able to estimate what the effect of a particular treatment\n",
        "would be on a patient. You'll also learn about\n",
        "the application of AI to text for particular tasks like, question answering and for extracting labels from\n",
        "radiology reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0cPBjF8W3T5",
        "colab_type": "text"
      },
      "source": [
        "We'll be diving\n",
        "straight into building a deep learning model for the task of chest\n",
        "x-ray classification. Many of the ideas that you will learn through this example are broadly applicable across\n",
        "many medical imaging tests. We'll start by\n",
        "looking at three examples of medical diagnostic tasks where deep learning has achieved incredible performance. We'll then jump into\n",
        "the training procedure for building AI models\n",
        "for medical imaging. Finally, we'll look at\n",
        "the testing procedure for evaluating the performance of\n",
        "these models on real data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXGB7t48TnMc",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86515217-0e426d80-be35-11ea-9cab-7dc62a1c3a87.JPG\" width=\"45%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86515216-0c78aa00-be35-11ea-8d8c-822cda687061.JPG\" width=\"45%\" />\n",
        "\n",
        "Our first example is in **dermatology. Dermatology is the branch of medicine dealing with the skin.** One of the task dermatologists\n",
        "perform is to look at a suspicious region\n",
        "of the skin to determine whether a mole\n",
        "is skin cancer or not. Early detection could likely have an enormous impact on\n",
        "skin cancer outcomes. The five-year survival rate\n",
        "of one kind of skin cancer drops significantly if\n",
        "detected in its later stages. In this study, an algorithm\n",
        "is trained to determine whether a region of skin\n",
        "tissue is cancerous or not. Using hundreds of thousands of images and labels as input, a convolutional neural network can be trained to do this task. Once the algorithm\n",
        "has been trained, the algorithms predictions\n",
        "can be evaluated against the predictions of\n",
        "human dermatologists on a new set of images. In this study, it was\n",
        "found that the algorithm performed as well as\n",
        "the dermatologists did.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0YIRWmgPbM",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86516143-d2f76d00-be3b-11ea-973a-e8b16b8ea37a.JPG\" width=\"45%\" height=\"20%\"/>\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86516144-d4289a00-be3b-11ea-8681-a77a6a06d9c4.JPG\" width=\"45%\" height=\"20%\" />\n",
        "\n",
        "Our second example\n",
        "is an **ophthalmology, which deals with the diagnosis and treatment of eye disorders.** One well-known study in 2016 looked at retinal\n",
        "fundus images, which photographed\n",
        "the back of the eye. One disease or\n",
        "pathology to look at here is diabetic retinopathy, which is damage to\n",
        "the retina caused by diabetes and is a major\n",
        "cause of blindness. Currently, detecting DR is a time-consuming and\n",
        "manual process that requires a trained clinician\n",
        "to examine these photos. In this study, an algorithm was developed to\n",
        "determine whether patients had diabetic retinopathy by looking at such photos. This study used over 128,000 images of which only 30 percent\n",
        "had diabetic retinopathy. We'll look at this data\n",
        "imbalanced problem, which is prominent\n",
        "in medicine and in many other fields\n",
        "with real-world data. We'll see some methods for\n",
        "tackling this challenge. Similarly to the previous study, this study showed that\n",
        "the performance of the resulting algorithm was comparable to ophthalmologists. In the study, a majority vote of multiple ophthalmologists\n",
        "was used to set the reference\n",
        "standard or ground troops, which is a group of experts, best guess of a right answer. Later this week in the course, we'll look at how\n",
        "ground-truth can be set in such medical AI studies. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86516145-d4c13080-be3b-11ea-812c-5cdeed454c02.JPG\" width=\"45%\" height=\"20%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86516146-d4c13080-be3b-11ea-9544-2b9f80ca28ed.JPG\" width=\"45%\" height=\"20%\" />\n",
        "\n",
        "Our third example is\n",
        "in **histopathology, a medical specialty involving examination of tissues\n",
        "under the microscope.** One of the tasks that\n",
        "pathologists do is look at scanned microscopic images of tissue called whole slide images, and determine the extent to\n",
        "which a cancer has spread. This is important to\n",
        "help plan treatment, predict the course\n",
        "of the disease, and the chance of recovery. In one study in 2017, using only 270\n",
        "whole slide images, AI algorithms were developed and then evaluated\n",
        "against pathologists. It was found that\n",
        "the best algorithms performed as well as\n",
        "the pathologists did. Now in histopathology, the images are very\n",
        "large and cannot be fed directly into an algorithm without\n",
        "breaking them down. The general setup\n",
        "of these studies is that instead of\n",
        "feeding in one large, high resolution digital\n",
        "image of the slide, several patches are extracted at a high magnification and\n",
        "used to train a model. These patches are labeled\n",
        "with the original label of the whole slide image and then fed into a deep\n",
        "learning algorithm. In this way, the algorithm can be trained on hundreds of\n",
        "thousands of patches. In this course, you will apply a similar idea of breaking\n",
        "down a large image into smaller images\n",
        "for model training to the task of brain\n",
        "tumor segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXPpBlDYKrRn",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519386-0ea03000-be58-11ea-8ce8-5eb0278d9033.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519388-1069f380-be58-11ea-8746-adf2f73430c8.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "Now that you've seen some of the\n",
        "cutting-edge applications of deep learning to medical image classification problems,\n",
        "we'll look at how you can build your own deep learning model for\n",
        "the medical imaging task of using chest X-rays to detect multiple\n",
        "diseases with a single model. We'll walk through the process of training\n",
        "a model for chest X-ray interpretation, and look at the key challenges that\n",
        "you will face in this process, and how you can go about successfully tackle. We'll start by looking at the task\n",
        "of chest X-ray interpretation. The chest X-ray is one of the most\n",
        "common diagnostic Imaging procedures in medicine with about 2 billion chest\n",
        "X-rays that are taken for a year. Chest X-ray interpretation is critical for the detection of many diseases,\n",
        "including pneumonia and lung cancer which affect millions\n",
        "of people worldwide each year. Now a radiologist who is trained in\n",
        "the interpretation of chest X-rays looks at the chest X-ray,\n",
        "looking at the lungs, the heart, and other regions to look for\n",
        "clues that might suggest if a patient has pneumonia or\n",
        "lung cancer or another condition. Let's look at one abnormality\n",
        "called a mass looks like. And I'm not going to first\n",
        "define what a mass is but let's look at three chest\n",
        "X-rays that contain a mass and three chest X-rays that are normal. I can then show you a new\n",
        "chest X-ray here and ask you to identify\n",
        "whether there is a mass. You might be able to correctly identify\n",
        "that this chest X-ray contains a mass. And here's the mass that might look\n",
        "similar to things that you see in these images, but not similar to\n",
        "anything that you see in these images. The way you're learning is very similar\n",
        "to how we're going to teach an algorithm to detect mass. For our own reference, a mass is\n",
        "defined as a lesion or in other words damage of tissue seen on a chest X-ray as\n",
        "greater than 3 centimeters in diameter. Let's see how we can train our\n",
        "algorithm to identify masses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYfbS8kqP8Jg",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519389-1069f380-be58-11ea-8d74-072483ede777.JPG\" width=\"100%\" height=\"30%\"/>\n",
        "\n",
        "During training, an algorithm\n",
        "is shown images of chest X-rays labeled with whether they\n",
        "contain a mass or not. The algorithm learns using\n",
        "these images and labels. The algorithm eventually learns\n",
        "to go from a chest X-ray input to produce the output of whether\n",
        "the X-ray contains mass. And this algorithm can\n",
        "go by different names. You may have heard of the terms\n",
        "deep learning algorithm or model or neural network or\n",
        "convolutional neural network. The algorithm produces an output\n",
        "in the form of scores, which are probabilities that\n",
        "the image contains a mass. So the probability that this image\n",
        "contains a mass is outputted to be 0.48, and the probability for\n",
        "this image is outputted to be 0.51. When training has not started,\n",
        "these scores, these probability outputs are not\n",
        "going to match the desired label. Let's say the desired label for\n",
        "mass is 1, and for normal is 0. And you can see that 0.48\n",
        "is far off from 1 and 0.51 is far off from\n",
        "the desired label of 0. And we can measure this error\n",
        "by computing a loss function. A loss function measures the error\n",
        "between our output probability and the desired label. We'll look at how this loss\n",
        "is computed soon enough. Then, a new set of images and desired\n",
        "labels is presented to the algorithm as it learns to produce scores that are\n",
        "closer to the desired labels over time. Notice how this output probability\n",
        "is getting closer to 1, and this output probability\n",
        "is getting closer to 0.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519391-11028a00-be58-11ea-9e0b-954ebd11dd0d.JPG\" width=\"49%\" height=\"30%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519393-119b2080-be58-11ea-8d42-9255e59feb99.JPG\" width=\"49%\" height=\"30%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4d02iXMQAO8",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519394-119b2080-be58-11ea-8376-ead297127bd2.JPG\" width=\"45%\" height=\"40%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86519395-1233b700-be58-11ea-9498-3d365bd1c0f3.JPG\" width=\"45%\" height=\"40%\" />\n",
        "\n",
        "Typically in the\n",
        "medical AI examples we've seen in previous videos, hundreds of thousands of images are shown\n",
        "to the algorithm. This is a typical setup\n",
        "for image classification, which is a core task in the\n",
        "computer vision field where a natural image is input to an image classification\n",
        "algorithm, which says what is the object\n",
        "contained in the image. You may have seen deep\n",
        "learning algorithms that can do this. Our example of chest\n",
        "X-ray classification is similar in many ways to the\n",
        "image classification setup. There a few additional\n",
        "challenges which make training medical\n",
        "image classification algorithms more challenging, which we'll cover next. We'll talk about three\n",
        "key challenges for training algorithms\n",
        "on medical images; the class imbalance challenge, the multitask challenge and\n",
        "the dataset size challenge. For each challenge, we'll cover one or two techniques\n",
        "to tackle them. Let's start with the class\n",
        "imbalance challenge. So here's the challenge. There's not an equal\n",
        "number of examples of non-disease and disease\n",
        "in medical datasets. This is a reflection of the prevalence or the frequency of disease in the real-world, where we see that there are a lot more examples of\n",
        "normals than of mass, especially if we're looking at X-rays of a healthy population. In a medical dataset, you might see 100 times\n",
        "as many normal examples as mass examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmyHde-dQAi1",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520096-a86adb80-be5e-11ea-94f5-6a147fe46f82.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520098-a9037200-be5e-11ea-8c31-3880a460d2bb.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "\n",
        "This creates a problem for\n",
        "the learning algorithm would seize mostly\n",
        "normal examples. This yields a model\n",
        "that starts to predict a very low probability\n",
        "of disease for everybody and won't be able to identify when an\n",
        "example has a disease. Let's see how we can\n",
        "trace this problem to the loss function that we\n",
        "use to train the algorithm. We'll also see how we can modify this loss function in the\n",
        "presence of imbalanced data. This loss over here is called the binary cross-entropy\n",
        "loss and this measures the performance of a classification\n",
        "model whose output is between zero and\n",
        "one Let's look at an example to see how this\n",
        "loss function evaluates. Here we have an example of a chest x-ray that\n",
        "contains a mass, so it gets labeled with\n",
        "one and the algorithm outputs a probability of 0.2. Now, the 0.2 here\n",
        "is the probability according to the algorithm\n",
        "of Y being equal to 1, the probability that\n",
        "this example is a mass. So now, we can apply the loss function to compute\n",
        "the loss on this example. Now notice that our label is one, so we're going to\n",
        "use the first term. Our loss is negative\n",
        "log and then we're going to take the\n",
        "algorithm output, 0.2. So this evaluates to 0.70. So this is the loss\n",
        "that the algorithm gets on this particular example. Let's look at another example. This time a non-mask example, which would have a label of zero. Our algorithm outputs\n",
        "a probability of 0.7. Now, this time we're going to use this term of the loss rate\n",
        "here because Y equals 0. So now the loss is going to be negative log of the term\n",
        "PY equals 0 given X. We can get PY equals 0 given X using P of Y equals 1 given X. The way we can compute\n",
        "this quantity from that one is by recognizing that the probability that\n",
        "an example is zero is 1 minus the\n",
        "probability that it's 1. An example as either mass or not. So the algorithm says 70 percent probability\n",
        "that something is mass, then there's 30 percent\n",
        "probability it's not. So here we're going to\n",
        "plug in 1 minus 0.7, that's going to\n",
        "come out to 0.3 and this expression\n",
        "evaluates to 0.52.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EWmKFZo7Qu5",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520101-a9037200-be5e-11ea-81a1-ad7c4ed3006b.JPG\" width=\"45%\" height=\"50%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520103-a99c0880-be5e-11ea-9d81-50d722dc2f63.JPG\" width=\"45%\" height=\"50%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520105-aa349f00-be5e-11ea-8918-aae2594d848d.JPG\" width=\"45%\" height=\"50%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520106-aacd3580-be5e-11ea-95ac-21595caaadf2.JPG\" width=\"45%\" height=\"50%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rSEfCHbQAgi",
        "colab_type": "text"
      },
      "source": [
        "We've seen how the loss is\n",
        "applied to a single example. Let's see how it applies\n",
        "to a bunch of examples. Here we have six examples\n",
        "that are normal, and two examples that are mass. Note that P2, P3, P4 here, are the patient IDs. When the training hasn't started, let's say the algorithm produces an output probability of 0.5\n",
        "for all of the examples, the loss can then be computed\n",
        "for each of the examples. For a normal example, we're going to use negative\n",
        "log of 1 minus 0.5, which is going to\n",
        "come out to 0.3. For a mass example, we're going to use\n",
        "negative log of 0.5, which is also going\n",
        "to come out to 0.3. The total contribution to the loss from the mass examples, comes out to 0.3 times\n",
        "2, which is 0.6. While the total loss\n",
        "from the normal example, comes out to 0.3 times the 6 normal examples, which is 1.8. So notice how most of\n",
        "the contribution to the loss is coming from\n",
        "the normal examples, rather than from\n",
        "the mass examples. So the algorithm is optimizing its updates to get\n",
        "the normal examples, and not giving much relative\n",
        "weight to the mass examples. In practice, this doesn't\n",
        "produce a very good classifier. This is the class\n",
        "imbalance problem. The solution to the\n",
        "class imbalance problem is to modify the loss function, to weight the normal and the\n",
        "mass classes differently. W_p will be the weight we assign to the positive or to\n",
        "the mass examples, and w_n to the negative\n",
        "or normal examples. Let's see what happens when we weight the positive\n",
        "examples more. We want to weight the\n",
        "mass examples more, such that they can have an equal contribution\n",
        "overall to the loss, as the normal examples. Let's pick six over eight as the weight we have on\n",
        "the mass examples, and two over eight\n",
        "as the weight we have on the normal examples. Then, you can see that if you sum up the total loss from\n",
        "the mass example, we get 0.45, and this is equal to the total loss from\n",
        "the normal examples here. In the general case, the weight we'll put on\n",
        "the positive class will be the number of negative examples over the total\n",
        "number of examples. In our case, this is six normal examples\n",
        "over a total examples. The weight we'll put\n",
        "on the negative class, will be the number of positive examples over the\n",
        "total number of examples, which is two over eight. With this setting of w_p and w_n, we can have over all\n",
        "of the examples for the loss contributions from the positive and the negative\n",
        "class to be the same. So this is the idea of modifying\n",
        "the loss using weights, in this method that's\n",
        "called the weighted loss, to tackle the class\n",
        "imbalance problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Vsd_bDQAeC",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520107-ab65cc00-be5e-11ea-87cb-56ab871ca8d7.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520108-ab65cc00-be5e-11ea-8957-b9bcdcf3f8eb.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "Let's discuss another\n",
        "procedure that we can use to tackle the class imbalance\n",
        "problem, which is re-sampling. The basic idea here is to\n",
        "re-sample the dataset such that we have an equal number\n",
        "of normal and mass examples. Let's see how we\n",
        "can achieve this. First, we group the normal and the mass\n",
        "examples together. Notice that the normal group has six examples and the\n",
        "mass has two examples. Now from these groups, we will sample the\n",
        "images such that there's an equal number of positive\n",
        "and negative samples. We can do this by sampling\n",
        "half of the examples from the positive or mass\n",
        "class and half of the examples from the\n",
        "negative or normal class. Note that this means\n",
        "we may not be able to include all of the normal\n",
        "examples in our re-sample. Furthermore, we may have more than one copy of the mass examples in\n",
        "our re-sampled dataset. With this re-sampled dataset, if we now compute the loss in the same way\n",
        "that we did before, we can see that even\n",
        "without the weights, this is just our standard\n",
        "binary cross-entropy loss. We see there's an equal\n",
        "contribution to the loss from the mass examples and\n",
        "from the normal examples. There are many variations\n",
        "of this approach, like under-sampling\n",
        "the normal class or oversampling the mass class, these approaches fall under the category of\n",
        "re-sampling methods, which we can use to combat\n",
        "the data imbalance problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owxfI3uQAbU",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520081-a30d9100-be5e-11ea-8318-c7a9e71c7957.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520083-a43ebe00-be5e-11ea-8a1d-cd1494678bea.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520084-a4d75480-be5e-11ea-8f51-4181fe54aad3.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520085-a56feb00-be5e-11ea-9d40-7aecb6073647.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "Let's chat about another\n",
        "challenge that we encounter in the medical\n",
        "image classification setting, which is the multitask challenge. This far, we have looked\n",
        "at binary classification, where we care about\n",
        "classifying whether an example is a\n",
        "mass or not a mass. However, in the real-world, we care about classifying\n",
        "the presence or absence of many such diseases. Now, one simple way to do this, is to have models that each\n",
        "learn one of these tasks. However, maybe we can learn to do all of the tasks\n",
        "using one model. An advantage of this\n",
        "is that we can learn features that are common to identifying more\n",
        "than one disease, allowing us to use our existing\n",
        "data more efficiently. This is the setup of\n",
        "multitask learning. Let's look at how we can\n",
        "train the algorithm to learn all these tasks\n",
        "at the same time. So instead of the examples\n",
        "having one label, they now have one label for every disease in\n",
        "the example where zero denotes the absence of that disease and one denotes the presence\n",
        "of that disease. For the first one, we\n",
        "have an absence of mass, a presence of pneumonia and an absence of another disease, edema, which is excess\n",
        "fluid in the lungs. Instead of having one\n",
        "output from the model, the model now has three\n",
        "different outputs denoting the probability of\n",
        "the three different diseases. To train such an algorithm, we also need to make\n",
        "the modification to the loss function from the binary tasks to\n",
        "the multitask setting. Let's look at how we can do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhyzOfkQAZd",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520087-a56feb00-be5e-11ea-8934-ae96350a622b.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520088-a6088180-be5e-11ea-8ece-a7eb81b93f11.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "We modify the loss function\n",
        "such that we look at the error associated\n",
        "with each disease. We can represent our new loss as the sum of the losses over\n",
        "the multiple diseases. This is called the multi-label loss or the multi-task loss. In this case, here's\n",
        "the loss that we get for the eight examples. This first term over here represents the loss\n",
        "associated with the mass class using this prediction probability\n",
        "and using this label. Similarly, here we have\n",
        "the loss associated with the pneumonia class coming with this model output and this label. So we add up the three\n",
        "losses given to us by the individual loss\n",
        "function components. One final consideration\n",
        "is how we can account for class imbalance\n",
        "in the multitask setting. Once again, we can apply the weighted loss that\n",
        "we have covered earlier. This time, we not\n",
        "only have a weight associated with just the positive and the negative labels, but it's for the positive label associated with that\n",
        "particular class and the negative label\n",
        "associated with that particular tasks\n",
        "such that for mass, there will be a different\n",
        "way to the positive class than to pneumonia or to edema. This covers their solution to the second challenge\n",
        "of multitask learning. Let's look at the\n",
        "third challenge which is the data set size challenge. For many medical\n",
        "imaging problems, the architecture of choice is the convolutional\n",
        "neural network, also called a ConvNet or CNN. These are designed to process\n",
        "2D images like x-rays. But variants of these\n",
        "are also well suited to medical signal processing\n",
        "or 3D medical images like CT scans, which we will look\n",
        "at in a future week. Several convolutional neural\n",
        "network architectures, such as Inception,\n",
        "ResNet, DenseNet, ResNeXt and EfficientNets\n",
        "have been proposed and are widely popular in\n",
        "image classification. These architectures are composed of various building blocks. In medical problems, the\n",
        "standard is to try out multiple models on\n",
        "the desired tasks and see which ones work best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC9FOg1-QAXq",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520089-a6a11800-be5e-11ea-8a3d-8ce42a64b886.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520091-a739ae80-be5e-11ea-838f-b7e50e6f683b.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "The challenge is that all of these architectures are\n",
        "data hungry and benefit from the millions of examples found in image\n",
        "classification datasets. In medical problems,\n",
        "how can we still apply these techniques when we don't\n",
        "have millions of examples? One solution, is to\n",
        "pre-train the network. Here the idea is to\n",
        "first have the network, look at natural images, and learn to identify objects\n",
        "such as penguins, or cats, or dogs, then use this\n",
        "network as a starting point for learning in\n",
        "medical imaging task by copying over the\n",
        "learned features. The network can then further\n",
        "be trained to look at chest X-rays and identify the presence and\n",
        "absence of diseases. The idea of this process, is that when we're learning our first task of\n",
        "identifying cats or dogs, the network will learn\n",
        "general features that will help it's learning\n",
        "on the medical task. An example of this, might be that the features\n",
        "that are useful to identify the edges on a penguin, are also useful for\n",
        "identifying edges on a lung, which are then helpful to\n",
        "identify certain diseases. Then when we transfer these\n",
        "features to our new network, the network can learn\n",
        "the new task of chest X-ray interpretation\n",
        "with a better starting point. This first step, is\n",
        "called pre-training, and the second step,\n",
        "is called fine-tuning. It is generally understood that the early layers\n",
        "of the network, capture low-level image features that are broadly generalizable, while the later layers\n",
        "capture details that are more high-level or more\n",
        "specific to a task. So for instance, the\n",
        "early layer might learn about the edges of an object, and this might be useful for chest X-ray interpretation later. But the later layers, might learn how to\n",
        "identify the head of a penguin and may not be useful for chest\n",
        "X-ray interpretation. So when we fine-tune the\n",
        "network on chest X-rays, instead of fine-tuning all\n",
        "features we've transferred, we can freeze the\n",
        "features learned by the shallow layers and just\n",
        "fine-tune the deeper layers. In practice, two of the most common design\n",
        "choices are one, to fine-tune all of the layers, and two, only\n",
        "fine-tune the later or the last layer and not\n",
        "fine-tune the earlier layers. This approach of pre-training\n",
        "and fine-tuning, is also called transfer\n",
        "learning and is an effective way to tackle the small dataset size challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvfS1SfQAVy",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520093-a739ae80-be5e-11ea-913e-d761264fe0d9.JPG\" width=\"45%\" height=\"30%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520094-a7d24500-be5e-11ea-9ccf-1ff722e61f90.JPG\" width=\"45%\" height=\"30%\" />\n",
        "\n",
        "Let's talk about\n",
        "a second solution to the data set size challenge. The idea is to trick the network into\n",
        "thinking that we have more training examples that are disposal than we actually do. Right before we pass an X-ray\n",
        "image into the network, we can apply a\n",
        "transformation to it. We have several options here. We can rotate it and pass it in, or we can translate it\n",
        "sideways and pass it in, or we can zoom in, or we can change the\n",
        "brightness or contrast, or apply a combination\n",
        "of these transformation. This method is called\n",
        "data augmentation. In practice, there are\n",
        "two questions that drive the choice of\n",
        "transformations we pick. The first is whether we believe the transformation\n",
        "reflects variations that will help the\n",
        "model generalize the test set and therefore\n",
        "the real world scenarios. For instance, we might believe\n",
        "that we're likely to see variations in contrast\n",
        "in natural X-rays, so we might have a\n",
        "transformation that changes the contrast of an image. A second design\n",
        "choice is to verify that our transformation\n",
        "keeps the label the same. For instance, if we're laterally inverting\n",
        "a patient's X-ray, this means flipping\n",
        "the left over to the right and the right\n",
        "over to the left, then their heart would appear on the left-hand\n",
        "side of the image. This is the right of the body. However, the label of normal would no longer hold\n",
        "because this is actually a rare heart condition\n",
        "called dextrocardia in which your heart points\n",
        "to the right side of your chest instead\n",
        "of to the left side. So this is not a transformation\n",
        "that preserves the label. The key here is we want the\n",
        "network to learn to recognize images with these transformations that still have the same label, not a different one. Beyond X-rays, there are other useful data\n",
        "augmentation procedures for other tasks. Rotation and flipping\n",
        "are useful for algorithms trained to detect\n",
        "skin cancer, for instance. In histopathology, one large source of\n",
        "real world variation is different shades\n",
        "of pink and purple seen in these microscopic images. Color noise is often added\n",
        "and all this does is have slightly different shades of these pink and purple colors to help the network generalize. In addition, rotation\n",
        "and cropping are also useful data\n",
        "augmentation procedures for histopathology images. As a recap, we've looked\n",
        "at the weighted loss and the resampling methods to tackle the class\n",
        "imbalance problem. We've looked at the\n",
        "multi-label loss to allow the network to identify multiple diseases\n",
        "in a chest X-ray. We've also covered transfer learning and data\n",
        "augmentation procedures as ways to tackle the challenge of having a small\n",
        "training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLQma19_ZmQ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520482-73608800-be62-11ea-902d-39d3cb5c51c4.JPG\" width=\"45%\" height=\"33%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520483-73f91e80-be62-11ea-9907-020d56294602.JPG\" width=\"45%\" height=\"33%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520485-7491b500-be62-11ea-8914-007787d8e652.JPG\" width=\"45%\" height=\"33%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520486-7491b500-be62-11ea-9dfd-caa27f389082.JPG\" width=\"45%\" height=\"33%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520487-752a4b80-be62-11ea-9c4a-eec0c1c3fa47.JPG\" width=\"45%\" height=\"33%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520481-7196c480-be62-11ea-952a-22edae5fc311.JPG\" width=\"45%\" height=\"33%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDPtB3zadsRI",
        "colab_type": "text"
      },
      "source": [
        "Now that you've seen how you would\n",
        "go about training a model for medical diagnosis, let's talk about how\n",
        "you would go about testing such a model. You will learn about the proper use of\n",
        "training, validation, and test sets. And about the need for strong ground\n",
        "truth in order to evaluate your models. When we apply machine\n",
        "learning to a dataset, we usually split it into a training and\n",
        "a test set. Our training set is used for the\n",
        "development and selection of models and our test set for\n",
        "the final reporting of our results. In reality, the training dataset is\n",
        "further split into a training set and a validation set, where the training\n",
        "set is used to learn a model and the validation set is used for\n",
        "hyper parameter tuning and giving an estimate of the model\n",
        "performance on the test set. Sometimes the split into a training and validation set is done multiple times\n",
        "in a method called cross validation to reduce variability in our\n",
        "estimate of the model performance. These sets also go by different names\n",
        "sometimes like validation can be called tuning or depth set, the training set\n",
        "can be called the development set, and the test set can go by holdout or\n",
        "even more confusing the validation set. We will stick to the terms training,\n",
        "validation, and test set for our purposes. We'll cover three challenges with building\n",
        "these sets in the context of medicine. The first challenge relates to how\n",
        "we make these test sets independent, the second relates to how we sample them,\n",
        "and the third relates to how\n",
        "we set the ground truth. Let's cover the problem\n",
        "of patient overlap first. Let's say a patient comes in twice for an\n",
        "x-ray, once in June and once in November. Both times, they're wearing a necklace\n",
        "when they have their x-ray taken. One of their x-rays is sampled\n",
        "as part of the training set and the other as part of test. We train our deep learning model and find that it correctly predicts normal for\n",
        "the x-ray in the test set. The problem is that it's possible\n",
        "that the model actually memorized to output normal when it saw\n",
        "the patient with a necklace on. This is not hypothetical,\n",
        "deep learning models can unintentionally memorize training\n",
        "data, and the model could memorize rare or unique training data aspects of\n",
        "the patient, such as the necklace, which could help it get the right answer\n",
        "when testing on the same patient. This would lead to an overly\n",
        "optimistic test set performance, where we would think that our model\n",
        "is better than it actually is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGW0LQTjQAUE",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520555-6abc8180-be63-11ea-9763-14318a7e703d.JPG\" width=\"45%\" height=\"40%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86520556-6c864500-be63-11ea-8f67-1cbde9124982.JPG\" width=\"45%\" height=\"40%\" />\n",
        "\n",
        "To tackle this problem in our data set, we can make sure that a patient's\n",
        "X-rays only occur in one of the sets. Now, if the model memorizes\n",
        "the necklace on the patient, it does not help it achieve\n",
        "a higher performance on the test set because it\n",
        "doesn't see the same patient. Let's see this in action\n",
        "over all patients. When we split a data set\n",
        "in the traditional way, images are randomly assigned\n",
        "to one of the sets. Notice that this way we get X-rays\n",
        "that belong to the same patient in different sets. For instance, X-ray one belongs to\n",
        "patient 20 and is part of training. X-ray two also belongs to patient 20 and\n",
        "is part of validation. And X-ray zero also belongs to patient 20,\n",
        "which is part of test. This is the problem of patient overlap. Instead, when we split\n",
        "a dataset by patient, all of the X-rays that belong to\n",
        "the same patient are in the same set. For instance, X-ray,\n",
        "X-ray two, and X-ray zero, which all belong to patient 20,\n",
        "are all part of training. Here seven, eight, and nine,\n",
        "which are all part of patient 32, are all in the validation set and\n",
        "we don't see 20 and 32 here in the test. This way, we can make sure there's\n",
        "no patient overlap between the sets. This covers our solution\n",
        "to the first challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld6Rt8KwUeBa",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528009-f533ce00-bec1-11ea-9173-77779a9e45da.JPG\" width=\"45%\" height=\"50%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528011-f7962800-bec1-11ea-808a-b53755571058.JPG\" width=\"45%\" height=\"50%\" />\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528013-f82ebe80-bec1-11ea-9b6b-4eec0567887f.JPG\" width=\"45%\" height=\"50%\"/>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528014-f8c75500-bec1-11ea-834e-6d6729325f5d.JPG\" width=\"45%\" height=\"50%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roAcve7HQASZ",
        "colab_type": "text"
      },
      "source": [
        "Let's cover the second\n",
        "challenge of set sampling. Let's say we sampled a test\n",
        "set from the dataset. Sometimes, the size of the test set is\n",
        "a fraction of the full dataset like 10%. Other times, in human comparison studies, because the test set needs to be annotated\n",
        "by human readers, the bottleneck for the test set size is how many examples\n",
        "can readers be expected to read? Typically, test sets contain\n",
        "at least hundreds of examples in medical AI studies. The challenge with sampling a test\n",
        "set is that when we're randomly sampling a test set of hundreds\n",
        "of examples from the dataset, we might not sample any patients\n",
        "that actually have a disease. Here, we might not sample any examples\n",
        "where the label for mass is 1. Thus, we would have no way to actually\n",
        "test the performance of the model on these positive cases. This is especially a problem with\n",
        "medical data where we might already have a small dataset and\n",
        "not that many examples of each disease. One way that this is tackled when creating\n",
        "test sets is to sample a test set such that we have at least X% of\n",
        "examples of our minority class. Here, the minority class\n",
        "is simply the class for which we have few examples like here\n",
        "examples where mass is present. One common choice of X is 50%. So for sampling a dataset of 100 examples, we would have 50 examples of mass and\n",
        "50 of not mass. This ensures that the study will have\n",
        "sufficient numbers to get a good estimate of the performance of the model both\n",
        "on non-disease and on disease examples. Once we sample the test set, typically, the validation set is\n",
        "sampled next before training. Because we want our validation set to\n",
        "reflect the distribution in the test set, typically, the same\n",
        "sampling strategy is used. We might decide to have once again\n",
        "100 examples in the validation set of which 50 are mass and 50 are non-mass. Finally, the remaining patients can\n",
        "be included in the training set. Because the test and\n",
        "validation set have been artificially sampled to have\n",
        "a large fraction of mass examples, the training set will have a much\n",
        "smaller fraction of mass examples. You have seen that we can still train\n",
        "a model in the presence of imbalance data. So this covers our second\n",
        "challenge of set sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zxfovFdQANZ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528131-e1d53280-bec2-11ea-8647-3c4dba48ebe3.JPG\" width=\"45%\" height=\"40%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528132-e1d53280-bec2-11ea-95cf-118d2b8e0c12.JPG\" width=\"45%\" height=\"40%\" />\n",
        "\n",
        "One major question in testing a model is\n",
        "how we determine the correct label for an example. The right label is more commonly called\n",
        "the ground truth in the context of machine learning or the reference\n",
        "standard in the context of medicine. On a chest X-ray, differentiating\n",
        "between some diseases might be complex. We might have one expert\n",
        "say this is pneumonia, another experts say it's another disease. This is called inter-observer\n",
        "disagreement, and is common in medical settings. The challenge here is how we can\n",
        "set the ground truth required for the evaluation of algorithms in the\n",
        "presence of inter-observer disagreement. So how do we determine the ground\n",
        "truth in the presence of inter-observer disagreement? We can use the consensus voting method. The idea behind consensus voting\n",
        "is to use a group of human experts to determine the ground truth. In one setting, we can have three\n",
        "radiologists look at a chest X-ray and each determine whether there\n",
        "is pneumonia present or not. If two out of the three say, yes,\n",
        "then we would say the answer is yes. In general, the answer will be the\n",
        "majority vote of the three radiologists. Alternatively, we can have the three\n",
        "radiologists get into a room and discuss their interpretation until\n",
        "they reach a single decision, which can then be used\n",
        "as the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNX7iOywd_Jf",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528133-e26dc900-bec2-11ea-97de-b6f894c67019.JPG\" width=\"45%\" height=\"20%\" />\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528134-e3065f80-bec2-11ea-8412-2a6e2060bdd2.JPG\" width=\"45%\" height=\"20%\" />\n",
        "\n",
        "The second method is to use a more\n",
        "definitive test which provides additional information\n",
        "to set the ground truth. For example, to determine whether\n",
        "a patient has a mass using a chest x-ray, a more definitive test that\n",
        "can be performed is a CT scan. The CT scan shows the 3D structure\n",
        "of the potential abnormality, thus giving the radiologist\n",
        "more information. If a mass is confirmed on the CT, we can then assign that ground\n",
        "truth to the chest x-ray. In the dermatology study we saw earlier,\n",
        "the ground truth for the test set was determined\n",
        "by a skin lesion biopsy. This is a medical procedure in\n",
        "which a sample of the skin with the suspected cancer is removed and\n",
        "tested in a lab. The results of this test are then used to\n",
        "set the ground truth for the photo image. We've now looked at two methods\n",
        "to set the ground truth or reference standard, consensus voting and\n",
        "having a more definitive test. With the second method, the difficulty is that we might not\n",
        "have these additional tests available. Not everyone who gets a chest\n",
        "x-ray gets a CT scan, and not everyone who has a potentially\n",
        "suspicious skin lesion gets a biopsy. Thus having a reliable ground truth with\n",
        "an existing data set often has to use the first method of getting a consensus\n",
        "ground truth on the existing data, which is the strategy that\n",
        "many medical AI studies use. Thus we have covered our three\n",
        "challenges with algorithm testing and some solutions in the context of medicine. We've seen how we can split by patient in\n",
        "the creation of training validation and test sets, how we can sample\n",
        "a minimum percent of a minority class examples in the test set, and\n",
        "how we can use consensus voting or a more definitive test\n",
        "to set the ground truth. You've learned about\n",
        "examples of deep learning models for tasks across several fields of medicine,\n",
        "and have learned about all the techniques\n",
        "needed to both train and test your own high-performance model for\n",
        "chest x-ray interpretation. You'll get to implement these\n",
        "ideas in the assignment and have your own chest x-ray\n",
        "interpretation model.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53019228/86528130-e0a40580-bec2-11ea-92f5-6df7a3cacda6.JPG\" width=\"100%\" height=\"10%\" />"
      ]
    }
  ]
}